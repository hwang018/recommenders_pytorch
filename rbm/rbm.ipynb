{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load book rating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'ratings':'../dataset/BX-Book-Ratings.csv',\n",
    "          'users':'../dataset/BX-Users.csv',\n",
    "          'books':'../dataset/BX-Books.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149780, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM():\n",
    "\n",
    "    def __init__(self,\n",
    "            hidden_units=500,\n",
    "            keep_prob=0.7,\n",
    "            init_stdv=0.1,\n",
    "            learning_rate=0.004,\n",
    "            minibatch_size=100,\n",
    "            training_epoch=20,\n",
    "            display_epoch=10,\n",
    "            debug=False,\n",
    "            with_metrics=False,\n",
    "            seed=42\n",
    "    ):\n",
    "        # RBM parameters\n",
    "        self.Nhidden = hidden_units  # number of hidden units\n",
    "        self.keep = keep_prob  # keep probability for dropout regularization\n",
    "\n",
    "        # standard deviation used to initialize the weights matrices\n",
    "        self.stdv = init_stdv\n",
    "\n",
    "        # learning rate used in the update method of the optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # size of the minibatch used in the random minibatches training; setting to 1 correspoNds to\n",
    "        # stochastic gradient descent, and it is considerably slower.Good performance is achieved\n",
    "        # for a size of ~100.\n",
    "        self.minibatch = minibatch_size\n",
    "        self.epochs = training_epoch + 1  # number of epochs used to train the model\n",
    "\n",
    "        # number of epochs to show the mse error during training\n",
    "        self.display = display_epoch\n",
    "\n",
    "        # protocol to increase Gibbs sampling's step. Array containing the\n",
    "        # percentage of the total training epoch when the step increases by 1\n",
    "        self.sampling_protocol = sampling_protocol\n",
    "\n",
    "        # if true, functions print their control paramters and/or outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # if true, compute msre and accuracy during training\n",
    "        self.with_metrics = with_metrics\n",
    "\n",
    "        # Seed\n",
    "        self.seed = seed\n",
    "\n",
    "        log.info(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "\n",
    "        self.w = tf.get_variable(\n",
    "            \"weight\",\n",
    "            [self.Nvisible, self.Nhidden],\n",
    "            initializer=tf.random_normal_initializer(stddev=self.stdv, seed=self.seed),\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        self.bv = tf.get_variable(\n",
    "            \"v_bias\",\n",
    "            [1, self.Nvisible],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        self.bh = tf.get_variable(\n",
    "            \"h_bias\",\n",
    "            [1, self.Nhidden],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    def __init__(self, nv, nh):\n",
    "        #weights used for the probability of the visible nodes given the hidden nodes (p_v_given_h))\n",
    "        # torch.rand : random normal distribution mean=0, variance=1 \n",
    "        self.W = torch.randn(nh,nv)\n",
    "        # bias probability of the hidden nodes given the visible nodes (p_h_given_v))\n",
    "        # fake dimension for the batch = 1, to align with 2d tensors\n",
    "        self.a = torch.randn(1,nh)\n",
    "        # bias probability of the visible nodes is activated \n",
    "        #given the value of the hidden nodes (p_v_given_h))\n",
    "        self.b = torch.randn(1, nv)\n",
    "        # we can add more params like learning rate... in this section to be used in train()\n",
    "\n",
    "    def sample_h(self, x):\n",
    "        # probability h is activated given the value v is the sigmoid(Wx+a).\n",
    "        # torch.mm make the product of two tensors. \n",
    "        # W.t()take the transpose because W is used for the p_v_given_h.\n",
    "        wx=torch.mm(x,self.W.t())\n",
    "        # .expand_as(wx) : expand the mini-batch.\n",
    "        activation=wx+self.a.expand_as(wx)\n",
    "        # probability p_h_given_v is the probability that the note drama genre is activated. \n",
    "        # v value is the input value. If v is a film drama, p_h_given_v will be hight. \n",
    "        # If v is not a film drama, p_h_given_v will be low.\n",
    "        p_h_given_v=torch.sigmoid(activation)\n",
    "        # Bernouilli RBM. we predict the user loves the movie or not (0 or 1).\n",
    "        # activation or not activation of the nh neurons. \n",
    "        return p_h_given_v, torch.bernouilli(p_h_given_v)\n",
    "\n",
    "    def sample_v(self, y): \n",
    "        # probability h is activated given the value v is the sigmoid(Wx+a).\n",
    "        # torch.mm make the product of two tensors. \n",
    "        wy=torch.mm(y,self.W)\n",
    "        # .expand_as(wx) : expand the mini-batch.\n",
    "        activation=wy+self.b.expand_as(wy)\n",
    "        p_v_given_h=torch.sigmoid(activation)\n",
    "        # Bernouilli RBM. we predict the user loves the movie or not (0 or 1).\n",
    "        # activation or not activation of the nv neurons. \n",
    "        return p_v_given_h, torch.bernouilli(p_v_given_h)\n",
    "\n",
    "        # Contrastive divergence Algorithm\n",
    "        # Optimize the weights to minimize the energy.\n",
    "        # ~ Maximize the Log-Likelihood of the model. \n",
    "        # Need to approximate the gradients with the algorithm contrastive divergence. \n",
    "    def train(self,v0,vk,ph0,phk):\n",
    "        #ph0,phk regarding to 1 user, this train function\n",
    "        # vk: visible nodes after k round trips of sampling\n",
    "        # ph: ph0: vector prob at first iteration of hidden nodes = 1, given v0\n",
    "        # phk vector prob at k iteration, for h = 1 given vk\n",
    "        \n",
    "        self.W += torch.mm(v0.t(),ph0)-torch.mm(vk.t(),phk)\n",
    "        # add ,0 for the tensor of two dimension \n",
    "        self.b += torch.sum((v0-vk),0) #keep format of v as 2d dim\n",
    "        self.a += torch.sum(ph0-phk,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2 : Create the RBM Object \n",
    "# number of movies\n",
    "nv=len(training_set[0]) \n",
    "# parameter is tunable is the number of features that we want to detect \n",
    "#hidden nodes rep some features rbm it to learn\n",
    "# features ~ genre, actors, director, oscar, date.... \n",
    "nh=100 #to tune\n",
    "# update the weights after serveral observations, also tunable\n",
    "batch_size=100 # each batch train how many samples\n",
    "# Creation of the object of the class RBM()\n",
    "rbm=RBM(nv,nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-10457187969e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#taking a batch of users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mid_user\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_users\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#range(a,b,c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# at the beginning v0=vk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# vk is going to be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_users' is not defined"
     ]
    }
   ],
   "source": [
    "### Part 3 : Training the RBM \n",
    "nb_epoch = 10 \n",
    "# upper bound is no included nb_epoch+1 \n",
    "\n",
    "# First for loop : epoch for loop \n",
    "for epoch in range (1,nb_epoch+1):\n",
    "    #loss function initialized to 0 at the beginning of the trainning \n",
    "    train_loss = 0\n",
    "    # counter which is a float . \n",
    "    s = 0.\n",
    "\n",
    "    # Second for loop : user forloop \n",
    "    # 0 lower bound \n",
    "    # nb_users-batch_size upper bound \n",
    "    # batch_size is the step of each batch (100)\n",
    "    # First batch is from user id=0 ti user id =99\n",
    "    \n",
    "    #taking a batch of users\n",
    "    for id_user in range(0,nb_users-batch_size,batch_size): #range(a,b,c)\n",
    "        # at the beginning v0=vk \n",
    "        # vk is going to be updated\n",
    "        # id_user,id_user+batch_size ~id_user+100\n",
    "        \n",
    "        vk=training_set[id_user:id_user+batch_size] #output of gibbs sampling, now dealing with specific user\n",
    "        v0=training_set[id_user:id_user+batch_size]\n",
    "        #initial probability prob hidden node at start = 1 given original ratings \n",
    "        ph0, _ = rbm.sample_h(v0)\n",
    "        \n",
    "        \n",
    "        # k steps in the random walk\n",
    "        # Third for loop : Contrastive divergence\n",
    "        for k in range(10): #why 10? mcmc technique\n",
    "            #call sample_h on visible nodes, get the first sampling of the first hidden node\n",
    "            _,hk=rbm.sample_h(vk) #v0 is the target, will not change, we take vk\n",
    "            #next we use the obtained hk to get the sampled vk\n",
    "            _,vk=rbm.sample_v(hk) #update vk, second sample of the visible nodes, until all the k steps are sampled and updated vk\n",
    "            \n",
    "            #vk are visible nodes in the kstep random walk\n",
    "            \n",
    "            #above are the actual sampled values \n",
    "            \n",
    "            # we don't want to learn where is no rating by the user\n",
    "            # no update when -1 rating.\n",
    "            vk[v0<0]=v0[v0<0] #training are not done on these -1 ratings\n",
    "            \n",
    "            phk,_=rbm.sample_h(vk) #get the probability of hk, last sample of the visible nodes in the kstep random walk, vk is the input, using this to get the phk\n",
    "            \n",
    "            # maximum likelihood to update the parameters, no return type\n",
    "            rbm.train(v0,vk,ph0,phk)\n",
    "            \n",
    "            # Contrastive divergence to approximate the gradient\n",
    "            \n",
    "            # Compare vk updated after the training to v0 the target. \n",
    "            # simple distance in absolute value \n",
    "            # [vO>=0] take only the value with ratings / coherence with vk[v0<0]=[v0<0]\n",
    "            #update the loss value\n",
    "            \n",
    "            # difference between target v0, and the prediction (last sample of the visible node of the contrastive divergence random walk)\n",
    "            # mean absolute difference between truth and predictions\n",
    "            train_loss+=torch.mean(torch.abs(v0[vO>=0]-vk[vO>=0]))\n",
    "            \n",
    "            #rmse version\n",
    "            #train_loss+=np.sqrt(torch.mean((v0[v0>=0] - vk[v0>=0])**2)) # RMSE here\n",
    "            s += 1.\n",
    "    # in each epoch , see overall loss function        \n",
    "    print('epoch: ' +str(epoch) +' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making prediction on the test set\n",
    "# testing RBM\n",
    "# mcmc\n",
    "\n",
    "test_loss = 0\n",
    "# counter which is a float . \n",
    "s = 0. #each step increment by 1\n",
    "\n",
    "#taking a batch of users, no need batch in testing\n",
    "for id_user in range(nb_users):\n",
    "    # v is input, need to keep training set, input used to activate the neurons to activate hidden states\n",
    "    # use training set input ratings to activate the neurons and to make predictions on the testset\n",
    "    v=training_set[id_user]\n",
    "    \n",
    "    #vt is the target, original ratings of testset\n",
    "    vt=test_set[id_user]\n",
    "    \n",
    "    # not consider rating of -1 in testset\n",
    "    if len(vt[vt>=0]) >0:\n",
    "        # then to make prediction\n",
    "        # sample hidden nodes first \n",
    "        _,h = rbm.sample_h(v)\n",
    "        # then use the sampled hidden nodes h to sample visible nodes v\n",
    "        _,v = rbm.sample_v(h)\n",
    "        \n",
    "        test_loss+=torch.mean(torch.abs(vt[vt>=0]-v[v>=0])) # excluding places where there are no rating, for fair comparison\n",
    "        #rmse version\n",
    "        #test_loss += np.sqrt(torch.mean((vt[vt>=0] - v[vt>=0])**2)) # RMSE here\n",
    "        s += 1.\n",
    "    print('test loss:'+str(test_loss/s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
